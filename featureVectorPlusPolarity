import re
import math
import nltk

''' HMM using initial featureVector plus polarity as extra featture , benchmark is simple getPolarity
errorRate for HMM: 0.488897396631
errorRate for benchmark: 0.504594180704
on training data.

On validation data (30% of training data), I get:
errorRate for HMM: 0.558098591549
errorRate for benchmark: 0.56161971831

  With my new strip function that uses pos tagging to strip sentences of all words except VB, RB and JJ, I get:
errorRate for HMM: 0.526408450704
errorRate for benchmark: 0.56514084507

  With new polarity function getPolarity3, without strip():
errorRate for HMM: 0.593309859155
errorRate for benchmark: 0.56161971831

  With new polarity function getPolarity3 AND strip():
errorRate for HMM: 0.549295774648
errorRate for benchmark: 0.628521126761

Conclusion: 5 features (plus getPolarity() ) + strip() is the best I have seen thus far on validation set
'''

numFeatures= 6
unknownTag= "<UNKN>"
numValuesPerFeature= 2
specialLastFeature= 3

# Karl
#given a string, outputs a list of tokens.
def tokenize(string):
    #add spaces around ! in order to treat them as tokens later.
    #removes other punctuation/special chars
    string= string.replace("\n", "")
    string= string.replace(",", "")
    string= string.replace(";", "") 
    string= string.replace(":", "")
    string= string.replace(";", "")
    string= string.replace("(","")
    string= string.replace(")","")
    string= string.replace("!"," ! ")
    string= string.replace("?", "")
    string= string.replace("-","")
    string= string.replace("\"", "")                
    string= string.replace("\'", "")
    string= string.replace(".", "")
    string= string.replace("_", "")
    string= string.replace("/", "")
    string= string.replace("\\", "")
    string= string.replace("$", "")
    string= string.replace("*", "")
    string= re.sub("[\s]+", " ", string)#to avoid empty string tokens
    #string= string.lower()
    #anything isolated by spaces is treated as a token.

    tokens = string.split(" ")


    return tokens

''' Karl
 '@param featureVector - list of 0s and 1s
 '@param sentiment - 1, 0 or -1
 '@return string of entries in featureVector followed by sentiment
 '@spec helper
 '@caller getFeatureCounts
 '''
def getHashable(featureVector, sentiment):
    hashable=""
    for feat in featureVector:
        hashable+= str(feat)
    hashable+= str(sentiment)
    return hashable
    
def getStructure( hashable ):
    hashable= list(hashable)
    featV= []
    for num in hashable:
        featV.append( int( num ) )
    sentiment= featV.pop()
    return featV,sentiment


# Karl
#@param tokens - string list of words
#@return string list of words with stopwords removed
def stopWordsFilter( tokens ):
    stopwords= ["the","of","for","a","an","as","to","from"]
    filtered= []
    for word in tokens:
        if word.lower() not in stopwords:
            filtered.append(word)
    return filtered

# Karl
#@param reviewsDic - mapping: reviewName => (sentiments, sentences)
#@param threshold - threshold for determining pos/neg sentiment
#@return a list of positive words and another of negative words
#@calling stopWordsFilter, tokenize
def getPosNegWordList( reviewsDic, poswords, negwords, threshold ):
    posWords= poswords
    negWords= negwords
    posCounts= dict()
    negCounts= dict()
    allCounts= dict() #mapping: word => total number of occurrences of word
    for reviewName in reviewsDic:
        (sentiments, sentences)= reviewsDic[reviewName]
        i= 0
        length= len(sentiments)
        while i < length:
            sentiment, sentence= sentiments[i], sentences[i]
            tokens= sentence
            for word in tokens:
                try:
                    allCounts[word]+= 1
                except KeyError:
                    allCounts[word]= 1
                try:
                    if sentiment == 1:
                        posCounts[word]+= 1
                    elif sentiment == -1:
                        negCounts[word]+= 1
                except KeyError:
                    if sentiment == 1:
                        posCounts[word]= 1
                    elif sentiment == -1:
                        negCounts[word]= 1
            i+=1
        
    for word in posCounts:
        count= posCounts[word]
        if count / allCounts[word] > threshold:
            posWords.append(word.lower())
    
    for word in negCounts:
        count= negCounts[word]
        if count / allCounts[word] > threshold:
            negWords.append(word.lower())
            
    return posWords,negWords

# Karl
#@param sentence - target sentence
#@param threshold - 
#@return 1 on true, 0 on false
def isLong(sentence, threshold):
    if len(sentence) > threshold:
        return 1
    else:
        return 0

# Karl
#@param sentence - target sentence
#@return 1 if sentence has an all CAPS word,
#              0 otherwise
def containsAllCaps(sentence):
    for word in sentence:
        if word.isupper():
            return 1
            
    return 0

# Karl
#@param sentence- target sentece
#@return (1,num pos words) or 0,0 if sentence has no positive words
def containsPos(sentence, poswords):
    success= 0
    numPositives= 0
    for word in sentence:
        if word.lower() in poswords:
            success= 1
            numPositives+= 1
    return success, numPositives

# Karl
#@param sentence- target sentence
#@return (1,num neg words) or 0,0 if sentence has no negative words
def containsNeg(sentence, negwords):
    success= 0
    numNegatives= 0
    for word in sentence:
        if word.lower() in negwords:
           success=1
           numNegatives+= 1
    return success,numNegatives

# Karl
#@param sentence - target sentence
#@return 1 if ! is in sentence, 0 otherwise
def containsExclamation(sentence):
    if "!" in sentence:
        return 1
    return 0

#Karl
#@param sentence - target sentence
#@return int list of our desired features
def getFeatureVector( sentence, poswords, negwords, observationType):
    if observationType == "featureVector":
        length= isLong(sentence, 20)
        capitalization= containsAllCaps(sentence)
        haspositive,numPos= containsPos(poswords,sentence)
        hasnegative,numNeg= containsNeg(negwords, sentence)
        punctuation= containsExclamation(sentence)
        pol, dets= getPolarity( sentence, poswords, negwords )
        return [length, capitalization, haspositive, hasnegative, punctuation, pol], numPos, numNeg
    elif observationType == "polarity":
        pol, dets= getPolarity( sentence, poswords, negwords ) 
        return [pol], dets

''' Karl
 '@param reviewsDic - mapping: reviewName => (sentiments,sentences)
 '@param poswords - list of positive words
 '@param negwords - list of negative words
 '@return mapping1: strconcat(featureVector,sentiment) => num occurrences of that combination in training
                mapping2: (sentiment2, sentiment1) => num times sentiment2 immediately followes sentiment1
                mapping3: sentiment => num occurrences in training
                also returns the total number of sentences in reviewsDic
 '@calling getFeatureVector
 '''
def getCounts(reviewsDic, poswords, negwords):
    featureCounts= dict()
    transitionCounts= dict()
    startCounts= dict()
    numSentences= 0
    for reviewName in reviewsDic:
        sentiments,sentences= reviewsDic[reviewName]
        prevSentiment= -2 #for resetting transition counts between reviews
        i= 0
        length= len(sentiments)
        while i < length:
            sentiment,sentence= sentiments[i],sentences[i]
            #get numSentences
            numSentences+=1
            
            #get start counts
            try:
                startCounts[sentiment]+= 1
            except KeyError:
                startCounts[sentiment]= 1
            
            #get transition counts
            if prevSentiment == -2:
                #start fresh: new review
                prevSentiment= sentiment
            else:
                try:
                    transitionCounts[(sentiment,prevSentiment)]+= 1
                except KeyError:
                    transitionCounts[(sentiment,prevSentiment)]= 1
                prevSentiment= sentiment
            
            #get feature counts
 
            vector, numPos, numNeg= getFeatureVector(sentence, poswords, negwords, "featureVector" )
            #vector, dets= getFeatureVector(sentence, poswords, negwords, "polarity" )
            key= getHashable(vector, sentiment)
            try:
                featureCounts[key]+= 1
            except KeyError:
                featureCounts[key]= 1
            i+= 1
    return featureCounts, transitionCounts, startCounts,numSentences


''' Karl
 '@param featureVector - target feature vector as string
 '@featureCounts - mapping: 'featureVector' + 'sentiment' => num occurrences
 '@return total occurrences of 'featureVector' in featureCounts
 '@note helper
 '@caller getProbabilities
 ''' 
def getFeatureCountsFor(featureVector, featureCounts):
        #if featureVector == "<UNK>":
           # print "counts for UNK: " + str( featureCounts["<UNK>"] )
            #return featureCounts["<UNK>"]
            
        featVPos= featureVector + "1"
        featVNeg= featureVector + "-1"
        featVNeu= featureVector + "0"
        try:
            totalFeatCount= featureCounts[featVPos]
        except KeyError:
            totalFeatCount= 0
        try:
            totalFeatCount+= featureCounts[featVNeg]
        except KeyError:
            totalFeatCount+= 0
        try:
            totalFeatCount+= featureCounts[featVNeu]
        except KeyError:
            totalFeatCount+= 0
            
        return float( totalFeatCount )



''' Karl
 '@param featureCounts - Mapping: 'featureVector' + 'sentiment' => count
 '@param threshold - maximum N_c to smooth, where c <= threshold
 '@return smoothed featureCounts
 '''
def goodturing(featureCounts, threshold):
    #space of possible observations
    totalObservations= 3* (numValuesPerFeature**numFeatures )* specialLastFeature
    totalSeen= len(featureCounts)
    totalUnseen= totalObservations - totalSeen
    
    #get counts for each N_c, along with all keys with that count
    countsByN= dict()
    keysInN= dict()
    for key, count in featureCounts.iteritems():
        if count in countsByN:
            countsByN[count]+=1
            keysInN[count].append(key)
        else:
            countsByN[count]= 1
            keysInN[count]= [key]
            
    #get count for unseen feature vectors   
    featureCounts[unknownTag + "1"]= countsByN[1] / ( 3*float(totalUnseen) )
    featureCounts[unknownTag + "0"]= featureCounts[unknownTag + "1"]
    featureCounts[unknownTag + "-1"] = featureCounts[unknownTag + "1"]
    
    
    #update count for all keys in N_c where c <= threshold
    for c in xrange(1, threshold+1):
        if (c+1) not in countsByN:
            print "never saw: " + str(c+1)
            break
        newCount= (c+1)*countsByN[c+1] / float(countsByN[c])
        keysToUpdate= keysInN[c]
        for key in keysToUpdate:
            featureCounts[key]= newCount
            
    return featureCounts

def alternateGoodTuring(featureCounts, threshold):
    #space of possible observations
    totalObservations= 3* (numValuesPerFeature**numFeatures) * specialLastFeature
    totalSeen= len(featureCounts)
    totalUnseen= totalObservations - totalSeen
    if totalUnseen == 0: return featureCounts
    #get counts for each N_c, along with all keys with that count
    countsByN= dict()
    keysInN= dict()
    for key, count in featureCounts.iteritems():
        if count in countsByN:
            countsByN[count]+=1
            keysInN[count].append(key)
        else:
            countsByN[count]= 1
            keysInN[count]= [key]
            
    #get count for unseen feature vectors   
    featureCounts[unknownTag + "1"]= 1/ ( 3*float(totalUnseen) )
    featureCounts[unknownTag + "0"]= featureCounts[unknownTag + "1"]
    featureCounts[unknownTag + "-1"] = featureCounts[unknownTag + "1"]
    
    #update count for all keys in N_c where c <= threshold
    for c in xrange(1, threshold+1):
        if (c+1) not in countsByN:
            print "never saw: " + str(c+1)
            break
        newCount= (c+1)*countsByN[c+1] / float(countsByN[c])
        keysToUpdate= keysInN[c]
        for key in keysToUpdate:
            featureCounts[key]= newCount
            
    return featureCounts
    

''' Karl
  '@param featureCounts - mapping: 'featureVector' + 'sentiment' => times combination occurs
  '@param transitionCounts - mapping: (sentiment2,sentiment1) => times sent2 follows sent1
  '@param startCounts - mapping: sentiment => times it occurs
  '@param numSentences - total nnumber of sentences observed
  '@return probabily mappings for each of the input count mappings
  '@spec no smoothing
  '''
def getProbabilities( featureCounts, transitionCounts, startCounts, numSentences ):
    featureProbs= dict()
    transitionProbs= dict()
    startProbs= dict()
    
    #get starting probabilities: Prob(pos), Prob(neg), Prob(neu)
    numSentences= float(numSentences)
    for sentiment in startCounts:
        startProbs[sentiment]= startCounts[sentiment] / numSentences
        
    #get transition probabilities: e.g. Prob( pos | neg ) - (1,-1) in our schema
    for (sent2,sent1) in transitionCounts:
        transitionProbs[(sent2,sent1)]= transitionCounts[(sent2,sent1)] / float(startCounts[sent1])
    
    #get emission/feature probabilities
    for key in featureCounts:
        length= len(key)
        #if key == "<UNK>":
          #  featureVector= key
        if length == numFeatures + 1:
            featureVector= key[:len(key)-1]
        elif length == numFeatures + 2:         
            featureVector= key[:len(key)-1]
            if featureVector[len(featureVector)-1] == "-":
                featureVector= featureVector.replace("-","")
        elif length == numFeatures + 3:
            featureVector= key[:len(key)-2]
            
        counts= getFeatureCountsFor(featureVector, featureCounts)
        #counts is a float
        print "key is " + key +  ", count is " + str(counts)
        featureProbs[key]= featureCounts[key] / counts
        
    return featureProbs, transitionProbs, startProbs
    
''' Karl
 '@param sentiment - 'pos', 'neg', or 'neu'
 '@return 1, -1, or 0 respectively
 '''
def getSentimentNumber(sentiment):
    if sentiment == "pos":
        return 1
    elif sentiment == "neg":
        return -1
    else:
        return 0

''' Karl
 '@param path - absolute path to training data
 '@return mapping: reviewName => (sentiments, sentences)
 '@return list of reviewNames in order seen
 '''
def getTrainingData(path):
    trainingData= dict()
    reviewNames= []
    reviewStart= re.compile("^[a-zA-Z0-9]{1,}_((pos)|(neg)|(neu))_",re.IGNORECASE)
    with open(path) as f:
        line= f.readline()
        reviewName= ""
        while (line):
            if reviewStart.match(line):
                trainingData[line]= ([], [])
                reviewName= line
                reviewNames.append(line)
            elif line != "\n":
                #get sentiment and sentence
                temp= line.split("\t", 1)
                assert len(temp) == 2
               # print temp
                temp[0]= getSentimentNumber(temp[0])
                #add to dictionary
                sents, sentences=trainingData[reviewName]
                sents.append(temp[0])
                #store sentence as a list
                #sentences.append( stopWordsFilter(tokenize( temp[1] ) ) )
                sentences.append( strip ( temp[1] ) )
                trainingData[reviewName]= (sents,sentences)
            line= f.readline()
    return trainingData, reviewNames 

''' Karl
 '@param path - absolute path to file
 '@return list of pos/neg words
 'http://positivewordsresearch.com/list-of-positive-words/
 'http://www.enchantedlearning.com/wordlist/negativewords.shtml
 '''
def getInitPosNeg(path):
    positives= []
    with open(path) as f:
        line= f.readline()
        while (line):
            line= line.replace("\n","")
            line= line.replace("\xa0","")
            line= line.replace("\xc2","")
            line= line.replace("\xe2","")
            line= line.replace("\x80","")
            line= line.replace("\x93","")
            line= line.replace("\\","")
            line= line.replace("-", " " )
            line= line.strip()
            line= re.sub("[\s]+", " ", line)#to avoid empty string tokens
            line= line.split(",")
            for entry in line:
                words= entry.strip().split(" ")
                for word in words:
                    positives.append( word.strip().lower() )
            line= f.readline()
    return positives
    
#
def viterbi(observations, states, startProbs, transitionProbs, emissionProbs, poswords, negwords):
    #initialize probabilities for first observation
    firstObservation= observations[0]
    firstFeatureVector,numPos,numNeg= getFeatureVector(firstObservation, poswords, negwords, "featureVector")
    #firstFeatureVector,dets= getFeatureVector(firstObservation, poswords, negwords, "polarity")
    #firstFeatureVector,dets= getPolarity(firstObservation, poswords, negwords)
    #irstFeatureVector= [str(firstFeatureVector)]
    
    V = [{}]
    path = {}
    
    #get initial probabilities for first observation
    for sentiment in states:
        try:
            emissionP= emissionProbs[ getHashable(firstFeatureVector, sentiment) ]
        except KeyError:
            emissionP= emissionProbs[  unknownTag + str(sentiment) ]
        V[0][sentiment]= math.log( startProbs[sentiment] ) + math.log( emissionP )
        path[sentiment]= [sentiment]
       
    print V
    
    for t in range(1, len(observations)):
        V.append({})
        newpath = {}
        
        nextObs= observations[t]
        nextFeatureVector, numPos, numNeg= getFeatureVector( nextObs, poswords, negwords, "featureVector" )
        for sent2 in states:
            maxProb= -99999999999;
            maxState= -2;
            for sent1 in states:
                #overall probability of going from sent1 to sent2
                try:
                    emissionprob= emissionProbs[ getHashable( nextFeatureVector, sent2 ) ]
                except KeyError:
                    emissionprob= emissionProbs[ unknownTag  + str(sent2) ]
                
                #causes 1% improvement: errorRate goes from 56.9% to 55.8%
                '''if sent2 == 1:
                    if numPos > numNeg:
                        emissionprob= emissionprob *1.5
                elif sent2 == -1 and numNeg > numPos:
                    emissionprob= emissionprob * 1.5'''
                    
                #overAll= V[t-1][sent1] * transitionProbs[(sent2,sent1)] * emissionprob
                print "********************************************************************"
                print V[t-1][sent1], transitionProbs[(sent2,sent1)], emissionprob
                overAll= V[t-1][sent1] +  math.log ( transitionProbs[(sent2,sent1)]  ) + math.log( emissionprob )
                print "overAll is: " + str( overAll )
                print "max Prob is: " + str(maxProb)
                print " ////////////////////////////////////////////////////////////////////////////////////////////////"
                if overAll > maxProb:
                    #found more likely sequence of sentiments
                    print "IN CONDITIONAL, storing: " + str(overAll)
                    maxProb= overAll
                    maxState= sent1
                    
            V[t][sent2]= maxProb
            newpath[sent2]= path[maxState] + [sent2]
 
        path= newpath
        
    n= 0
    if len(observations) != 1:
        n=  t
    (prob, state) = max((V[n][y], y) for y in states)
    return (prob, path[state])

def classify( data, states, startProbs, transitionProbs, featureProbs, pos, neg ):
    results= dict()
    for reviewName in sorted(data.keys()):
        (sentiments, sentences)= data[reviewName]
        finalProb, path= viterbi( sentences, states, startProbs, transitionProbs, featureProbs, pos, neg )
        results[reviewName]= (finalProb, path)
    return results

def benchmark( data, pos, neg ):
    results= dict()
    for reviewName in sorted(data.keys()):
        (sentiments, sentences)= data[reviewName]
        partialResults= []
        for sentence in sentences:
            res, dets= getPolarity( sentence, pos, neg )
            partialResults.append(res)
        results[reviewName]= (0, partialResults)
    return results

def prepareTest( tests ):
    observations= []
    trueLabels= []
    for key in sorted(tests.keys()):
        print "getting info for key: " + key
        (labels,sentences)= tests[key]
        observations+= sentences
        trueLabels+= labels
    return observations, trueLabels

def differences(trueLabels, classifications):
    results= []
    for reviewName in sorted(classifications.keys()):
        (prob,path)= classifications[reviewName]
        results+= path
        
    tuples= []
    diffs= []
    for i in range(0,len(trueLabels)):
       tuples.append( (trueLabels[i],results[i]) )
       if trueLabels[i] != results[i]:
           diffs.append(1.0)
       else:
           diffs.append(0.0)
           
    errorRate= sum(diffs) / len(diffs)
    return tuples, diffs, errorRate

def getPolarity2( sentence, poswords, negwords ):
    polarities= []
    
    #initialize each word
    for word in sentence:
        word= word.lower()
        if word in poswords:
            polarities.append(1)
        elif word in negwords:
            polarities.append(-1)
        else:
            polarities.append(0)
            
    
    
    counts= [0,0,0]
    for res in polarities:
        counts[res+1]+= 1
    
    length=float(  len(counts) )
    for i in range(0,int(length)):
        counts[i]= counts[i]/length
    
    if math.fabs(counts[-1] - counts[1]) > 0.50:
        return counts.index( max( counts ) ) - 1, polarities
    else:
        return 0, polarities
    
    
def getPolarity( sentence, poswords, negwords ):
    polarities= []
    
    #initialize each word
    for word in sentence:
        #print word
        word= word.lower()
        if word in poswords:
            polarities.append(1)
        elif word in negwords:
            polarities.append(-1)
        else:
            polarities.append(0)
    
    counts= [0,0,0]
    for res in polarities:
        #print res
        counts[res+1]+= 1
    return counts.index( max( counts ) ) - 1, polarities

def getPolarity3( sentence, poswords, negwords ):
    polarities= []
    
    #initialize each word
    for word in sentence:
        #print word
        word= word.lower()
        if word in poswords:
            polarities.append(1)
        elif word in negwords:
            polarities.append(-1)
        #else:
         #   polarities.append(0)
    
    counts= [0,0,0]
    for res in polarities:
        #print res
        counts[res+1]+= 1
    polarity= counts[2] - counts[0]
    if polarity > 0:
        return 1, polarities
    elif polarity < 0:
        return -1, polarities
    else:
        return 0, polarities

def strip( sentence ):
    essence= []
    tokens = nltk.word_tokenize(sentence)
    tagged = nltk.pos_tag(tokens)
    targets= ["RB","JJ","VB","VBD","VBN","NN"]
    for word,tag in tagged:
        if tag in targets:
            essence.append(word.replace("n't","not"))
    return essence
    
    
def createKaggleSubmission( resultsDic, names ):
    with open("/Users/Karl/Dropbox/NLP final project/cs4740_P3/testResults.txt","w") as f:
        count= 0
        f.write("Id,answer\n")
        for reviewName in names:
            #f.write( reviewName )
            _,labels= resultsDic[reviewName]
            for val in labels:
                f.write( str(count) + "," + str(val) + "\n" )
                count+=1
            
def aggregate( sentenceResults ):
    reviewResults= []
    for reviewName in sorted(sentenceResults.keys()):
        _,labels= sentenceResults[reviewName]
        counts= [0,0,0]
        for res in labels:
            #print res
            counts[res+1]+= 1
        polarity= counts[2] - counts[0]
        if polarity > 0:
            polarity= 1     
        elif polarity < 0:
            polarity= -1
        else:
            polarity= 0
        reviewID= reviewName[reviewName.rindex("_") + 1:len(reviewName)-1]
        reviewResults.append( ( reviewID, polarity ) )
    return reviewResults

'''Running this file will execute the code below this line '''

ex, exRevNames= getTrainingData("/Users/Karl/Dropbox/NLP final project/cs4740_P3/training_data.txt")
validation, revNames= getTrainingData("/Users/Karl/Dropbox/NLP final project/cs4740_P3/validation.txt")
initialPos= getInitPosNeg("/Users/Karl/Dropbox/NLP final project/cs4740_P3/positives.txt")
initialNeg= getInitPosNeg("/Users/Karl/Dropbox/NLP final project/cs4740_P3/negatives.txt")
pos, neg= getPosNegWordList( ex, initialPos, initialNeg, 0.9)
featureCounts, transitionCounts, startCounts, numSentences= getCounts(ex,pos,neg)
smoothedCounts=  goodturing(featureCounts.copy(), 5)
#smoothedCounts=  alternateGoodTuring(featureCounts.copy(), 5)
featureProbs, transitionProbs, startProbs= getProbabilities(smoothedCounts, transitionCounts, startCounts, numSentences)
states= [-1,1,0]
#sentences, trueLabels= prepareTest ( validation )
#finalProb, path= viterbi( sentences, states, startProbs, transitionProbs, featureProbs, pos, neg )
#results= classify( validation , states, startProbs, transitionProbs, featureProbs, pos, neg )
#if len(trueLabels) == len(path):
 #   print "OK AS EXPECTED"
    
#tuples, diffs, errorRate= differences(trueLabels, results)
#print "errorRate for HMM: " + str(errorRate)
#results= benchmark( validation, pos, neg )
#tuples, diffs, errorRate= differences(trueLabels, results)
#print "errorRate for benchmark: " + str(errorRate)
print "************ PROCESS TEST SET ****************"
test, revNames= getTrainingData("/Users/Karl/Dropbox/NLP final project/cs4740_P3/test_data_no_true_labels.txt")
testResults= classify( test, states, startProbs, transitionProbs, featureProbs, pos, neg )
createKaggleSubmission( testResults, revNames )


pex1= "Great cut: Hot Summer Day: unique, superb mix of instruments (violin, harmonica, keyboard, and electric guitar), \
            nice vocals (the lyrics are kinda dumb, but that's part of the 60s), fantastic build up...everything that characterizes a great 60s song."
pex1= tokenize( pex1 )
pex2= tokenize( "Extra features after game play." )
nex1= tokenize( "No ability to change from English to Japanese in the music/subtitles." )
nex2= tokenize( "Another issue is the loss of the original design illustrations." )
nuex1= tokenize( "The original game had a spoiler for a cover." )
nuex2= tokenize( "The narrative is very realistic, and just when it seems that the story is reaching its climax, it ends!" )
rpex1= getPolarity2( pex1, pos, neg )
rpex2= getPolarity2( pex2, pos, neg )
rnex1= getPolarity2( nex1, pos, neg )
rnex2= getPolarity2( nex2, pos, neg )
rnuex1= getPolarity2( nuex1, pos, neg )
rnuex2= getPolarity2( nuex2, pos, neg )
